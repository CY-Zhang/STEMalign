{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seeing-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.nn import Module\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import glob\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "measured-parallel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cuda:0')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "652fa6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameter():\n",
    "    def __init__(self):\n",
    "        self.dict = {}\n",
    "        self.dict['optimizer'] = {}\n",
    "        self.dict['optimizer']['lr'] = [1e-4, 5.5e-6]\n",
    "        self.dict['optimizer']['beta'] = [(0.9, 0.999), (0.9, 0.999)]\n",
    "        self.dict['optimizer']['eps'] = [1e-7, 1e-7]\n",
    "        self.dict['optimizer']['method'] = 'Adam'\n",
    "    \n",
    "        self.dict['epoch'] = [100, 100]\n",
    "        self.dict['batch_size'] = 32\n",
    "        self.dict['patience'] = 5\n",
    "        self.dict['min_delta'] = 1e-4\n",
    "        \n",
    "        self.dict['training_data'] = {}\n",
    "        self.dict['training_data']['path'] = '/home/chenyu/Desktop/CNNmeasurement/TrainingData_CoarseCNN/'\n",
    "        self.dict['training_data']['data'] = 'FullRandom_50mrad_defocus_whiteNoise_32pxGaussian_Noiseless_128pxRonch_x20000.npy'\n",
    "        self.dict['training_data']['label'] = 'FullRandom_50mrad_defocus_highAbr_whiteNoise_32pxGaussian_Noiseless_defocus_x20000.npy'\n",
    "        self.dict['training_data']['split'] = 0.8\n",
    "        self.dict['training_data']['aperture'] = 45\n",
    "        self.dict['training_data']['limit'] = 50\n",
    "        self.dict['training_data']['blur'] = 0.7\n",
    "        \n",
    "        self.dict['segmentation'] = {}\n",
    "        self.dict['segmentation']['resize'] = (135, 135)\n",
    "        self.dict['segmentation']['ratio'] = (0.9, 1.1)\n",
    "        self.dict['segmentation']['scale'] = (0.9, 1.1)\n",
    "        self.dict['segmentation']['normalize'] = None\n",
    "        \n",
    "        \n",
    "        self.dict['architecture'] = {}\n",
    "        self.dict['architecture']['dropout'] = 0.3\n",
    "        self.dict['architecture']['linear_shape'] = [256]\n",
    "        \n",
    "        self.process = {}\n",
    "        \n",
    "        self.process['training'] = []\n",
    "        self.process['validation'] = []\n",
    "        \n",
    "    def add_training_process(self, training_acc, validation_acc, epoch):\n",
    "        if epoch == 0:\n",
    "            self.process['training'].append([])\n",
    "            self.process['validation'].append([])\n",
    "        if torch.is_tensor(training_acc):\n",
    "            training_acc = training_acc.cpu().detach().numpy()\n",
    "        if torch.is_tensor(validation_acc):\n",
    "            validation_acc = validation_acc.cpu().detach().numpy()\n",
    "        self.process['training'][-1].append(training_acc)\n",
    "        self.process['validation'][-1].append(validation_acc)\n",
    "        return\n",
    "    \n",
    "    def save_result(self, path):\n",
    "        with open(path + 'hyperparameter.pkl', 'wb') as f:\n",
    "            pickle.dump(self.dict, f)\n",
    "        with open(path + 'training_log.pkl', 'wb') as f:\n",
    "            pickle.dump(self.process, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd8f214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "par = Hyperparameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "connected-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized CNN model\n",
    "class Net(Module):   \n",
    "    def __init__(self, pretrained = False, dropout = 0.3, linear_shape = 512):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv7 = nn.Conv2d(256, 256, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv8 = nn.Conv2d(256, 512, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv9 = nn.Conv2d(512, 512, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv10 = nn.Conv2d(512, 512, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv11 = nn.Conv2d(512, 512, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv12 = nn.Conv2d(512, 512, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv13 = nn.Conv2d(512, 512, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.fc1 = nn.Linear(4 * 4 * 512, linear_shape)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "#         self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(linear_shape, 1)\n",
    "    \n",
    "    def lock_base(self):\n",
    "        for parameter in self.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        self.fc1.weight.requires_grad = True\n",
    "        self.fc1.bias.requires_grad = True\n",
    "#         self.fc2.weight.requires_grad = True\n",
    "#         self.fc2.bias.requires_grad = True\n",
    "        self.fc3.weight.requires_grad = True\n",
    "        self.fc3.bias.requires_grad = True\n",
    "    \n",
    "    def unlock_base(self):\n",
    "        for parameter in self.parameters():\n",
    "            parameter.requires_grad = True\n",
    "            \n",
    "    def load_pretrained(self):\n",
    "        print(\"Loading weights and bias from VGG16.\")\n",
    "        vgg16 = torchvision.models.vgg16(pretrained = True)\n",
    "        self.conv1.weight.data = vgg16.features[0].weight.data.to(device = device)\n",
    "        self.conv1.bias.data = vgg16.features[0].bias.data.to(device = device)\n",
    "        self.conv2.weight.data = vgg16.features[2].weight.data.to(device = device)\n",
    "        self.conv2.bias.data = vgg16.features[2].bias.data.to(device = device)\n",
    "        self.conv3.weight.data = vgg16.features[5].weight.data.to(device = device)\n",
    "        self.conv3.bias.data = vgg16.features[5].bias.data.to(device = device)\n",
    "        self.conv4.weight.data = vgg16.features[7].weight.data.to(device = device)\n",
    "        self.conv4.bias.data = vgg16.features[7].bias.data.to(device = device)\n",
    "        self.conv5.weight.data = vgg16.features[10].weight.data.to(device = device)\n",
    "        self.conv5.bias.data = vgg16.features[10].bias.data.to(device = device)\n",
    "        self.conv6.weight.data = vgg16.features[12].weight.data.to(device = device)\n",
    "        self.conv6.bias.data = vgg16.features[12].bias.data.to(device = device)\n",
    "        self.conv7.weight.data = vgg16.features[14].weight.data.to(device = device)\n",
    "        self.conv7.bias.data = vgg16.features[14].bias.data.to(device = device)\n",
    "        self.conv8.weight.data = vgg16.features[17].weight.data.to(device = device)\n",
    "        self.conv8.bias.data = vgg16.features[17].bias.data.to(device = device)\n",
    "        self.conv9.weight.data = vgg16.features[19].weight.data.to(device = device)\n",
    "        self.conv9.bias.data = vgg16.features[19].bias.data.to(device = device)\n",
    "        self.conv10.weight.data = vgg16.features[21].weight.data.to(device = device)\n",
    "        self.conv10.bias.data = vgg16.features[21].bias.data.to(device = device)\n",
    "        self.conv11.weight.data = vgg16.features[24].weight.data.to(device = device)\n",
    "        self.conv11.bias.data = vgg16.features[24].bias.data.to(device = device)\n",
    "        self.conv12.weight.data = vgg16.features[26].weight.data.to(device = device)\n",
    "        self.conv12.bias.data = vgg16.features[26].bias.data.to(device = device)\n",
    "        self.conv13.weight.data = vgg16.features[28].weight.data.to(device = device)\n",
    "        self.conv13.bias.data = vgg16.features[28].bias.data.to(device = device)\n",
    "    \n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = F.relu(self.conv10(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = F.relu(self.conv13(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "level-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the training data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rational-wayne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 128, 128)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "input_path = par.dict['training_data']['path']\n",
    "train_data = np.load(input_path + par.dict['training_data']['data'])\n",
    "print(train_data.shape)\n",
    "\n",
    "train_label = np.load(input_path + par.dict['training_data']['label'])\n",
    "train_label = (train_label - np.amin(train_label))/(np.amax(train_label) - np.amin(train_label))\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "multiple-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_range (input, min, max):\n",
    "    input += -(np.min(input))\n",
    "    input /= np.max(input) / (max - min)\n",
    "    input += min\n",
    "    return input    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe517879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aperture_generator(px_size, simdim, ap_size):\n",
    "    x = np.linspace(-simdim, simdim, px_size)\n",
    "    y = np.linspace(-simdim, simdim, px_size)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    apt_mask = mask = np.sqrt(xv*xv + yv*yv) < ap_size # aperture mask\n",
    "    return apt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3f61cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if par.dict['training_data']['aperture'] != 0:\n",
    "    aperture_mask = aperture_generator(128, par.dict['training_data']['limit'], \n",
    "                                       par.dict['training_data']['aperture'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "flying-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RonchigramDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform = None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index, :, :].astype('float')\n",
    "        img = scale_range(img, 0.05, 1.0)\n",
    "        \n",
    "        if par.dict['training_data']['aperture'] != 0:\n",
    "            img = img * aperture_mask\n",
    "            \n",
    "        if par.dict['training_data']['blur'] != 0:\n",
    "            img = gaussian_filter(img, sigma = par.dict['training_data']['blur'])\n",
    "            \n",
    "        img = scale_range(img, 0, 1)\n",
    "        new_channel = np.zeros(img.shape)\n",
    "        img = np.dstack((img, new_channel, new_channel))\n",
    "        img = Image.fromarray(np.uint8(img * 255))\n",
    "        y_label = torch.tensor(float(self.labels[index]))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "behind-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "        [\n",
    "#             transforms.Resize(par.dict['segmentation']['resize']),\n",
    "            transforms.RandomAutocontrast(p=0.5),\n",
    "            transforms.RandomRotation(90),\n",
    "            transforms.RandomResizedCrop((128, 128), scale = par.dict['segmentation']['scale'], ratio = par.dict['segmentation']['ratio']),\n",
    "            transforms.ToTensor(),\n",
    "            # TODO: need to add random shear here\n",
    "            # option to normalize a tensor with mean and standard deviation, similar to featurewise center in Keras\n",
    "#             transforms.Normalize((1.0, 1.0, 1.0), (1.0, 1.0, 1.0)),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hundred-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "train_CNN = False\n",
    "batch_size = par.dict['batch_size']\n",
    "shuffle = True\n",
    "pin_memory = True\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "embedded-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RonchigramDataset(train_data, train_label, transform = transform)\n",
    "\n",
    "n_train = int(len(train_data) * par.dict['training_data']['split'])\n",
    "n_val = len(train_data) - n_train\n",
    "train_set, validation_set = torch.utils.data.random_split(dataset,[n_train,n_val])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers,pin_memory=pin_memory)\n",
    "\n",
    "validation_loader = DataLoader(dataset=validation_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa974f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 19\n",
    "plt.imshow(images[idx,0,:,:], cmap = 'gray')\n",
    "plt.colorbar()\n",
    "print(labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "fig = plt.figure(figsize = [30,18])\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the CNN model and start training\n",
    "model = Net(dropout = par.dict['architecture']['dropout'], linear_shape = par.dict['architecture']['linear_shape'][0]).to(device)\n",
    "model.load_pretrained()\n",
    "model.lock_base()\n",
    "criterion = nn.MSELoss(reduction = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader == train_loader:\n",
    "        print(\"Checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on validation data\")\n",
    "\n",
    "    sum_MSE = 0\n",
    "    counter = 0\n",
    "    loss = nn.MSELoss(reduction = 'mean')\n",
    "    model.eval()\n",
    "    \n",
    "    # define two lists to save truth and predictions, for the plot only.\n",
    "    y_list = torch.empty(0).to(device = device)\n",
    "    pred_list = torch.empty(0).to(device = device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            pred = torch.squeeze(model(x))\n",
    "            pred_list = torch.cat((pred_list, pred), 0)\n",
    "            y_list = torch.cat((y_list, y), 0)\n",
    "            sum_MSE += loss(pred, y) * y.shape[0]\n",
    "            counter += y.shape[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize = (5,5))\n",
    "    img = ax.scatter(y_list.cpu().numpy(), pred_list.cpu().numpy(), s = 1)\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.plot(np.linspace(0,1,100), np.linspace(0,1,100),'--', c = 'red')\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "    ax.set_xlabel('Truth',fontsize = 16)\n",
    "    ax.set_ylabel('Prediction', fontsize = 16)\n",
    "    plt.show()\n",
    "    \n",
    "    model.train()\n",
    "#     print( f\"Got accuracy {float(sum_MSE)/float(counter):.6f}\" )   \n",
    "    return f\"{float(sum_MSE)/float(counter):.6f}\"\n",
    "    \n",
    "#     model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience = 5, min_delta = 0):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    early_stopping = EarlyStopping(patience = par.dict['patience'], min_delta = par.dict['min_delta'])\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        loop = tqdm(train_loader, total = len(train_loader), leave = True)\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            val_acc = check_accuracy(validation_loader, model)\n",
    "            loop.set_postfix(val_acc = val_acc)\n",
    "            early_stopping(float(val_acc))\n",
    "            if early_stopping.early_stop:\n",
    "                par.add_training_process(acc_loss / counter , float(val_acc), epoch)\n",
    "                break\n",
    "                \n",
    "        acc_loss = 0\n",
    "        counter = 0\n",
    "        \n",
    "        for imgs, labels in loop:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = torch.squeeze(model(imgs))\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc_loss += loss * labels.shape[0]\n",
    "            counter += labels.shape[0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loop.set_description(f\"Epoch [{epoch}/{num_epochs}]\")\n",
    "            loop.set_postfix(loss = loss.item())\n",
    "        \n",
    "        par.add_training_process(acc_loss / counter , val_acc, epoch)\n",
    "        print(f\"Training acc: {float(acc_loss) / float(counter):.6f}, Validation accuracy {float(val_acc):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49a5201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stage = 0\n",
    "learning_rate = par.dict['optimizer']['lr'][stage]\n",
    "num_epochs = par.dict['epoch'][stage]\n",
    "betas = par.dict['optimizer']['beta'][stage]\n",
    "eps = par.dict['optimizer']['eps'][stage]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, betas = betas, eps = eps)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed9c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stage = 1\n",
    "learning_rate = par.dict['optimizer']['lr'][stage]\n",
    "num_epochs = par.dict['epoch'][stage]\n",
    "betas = par.dict['optimizer']['beta'][stage]\n",
    "eps = par.dict['optimizer']['eps'][stage]\n",
    "model.unlock_base()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, betas = betas, eps = eps)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d23293",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,5])\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(np.array(par.process['training'][0]), '.', markersize = 10 , label = 'training')\n",
    "ax.plot(np.array(par.process['validation'][0], dtype = 'float'), '.', markersize = 10 , label = 'validation')\n",
    "ax.set_yscale('log')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(which='major',direction = 'in', length = 10, width = 2)\n",
    "plt.tick_params(which='minor',direction = 'in', length = 5)\n",
    "ax.set_xlabel('Training Epoch',fontsize = 16)\n",
    "ax.set_ylabel('Normalized RMSE', fontsize = 16)\n",
    "plt.legend(fontsize = 16)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(np.array(par.process['training'][1]), '.', markersize = 10 , label = 'training')\n",
    "ax.plot(np.array(par.process['validation'][1], dtype = 'float'), '.', markersize = 10 , label = 'validation')\n",
    "ax.set_yscale('log')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tick_params(which='major',direction = 'in', length = 10, width = 2)\n",
    "plt.tick_params(which='minor',direction = 'in', length = 5)\n",
    "ax.set_xlabel('Training Epoch',fontsize = 16)\n",
    "ax.set_ylabel('Normalized RMSE', fontsize = 16)\n",
    "plt.legend(fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'TorchModels/Test18_Nion_defocus_45mradApt_50mradLimit_emit+defocus_Adam_attempt05.pt')\n",
    "par.save_result('TorchModels/Test18_Nion_defocus_45mradApt_50mradLimit_emit+defocus_Adam_attempt05_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c87cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"TorchModels/Test18_Nion_defocus_45mradApt_50mradLimit_emit+defocus_Adam_attempt05_hyperparameter.pkl\",'rb')\n",
    "object_file = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ea5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"TorchModels/Test18_Nion_defocus_45mradApt_50mradLimit_emit+defocus_Adam_attempt05_training_log.pkl\",'rb')\n",
    "object_file = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0debf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(object_file['training'][0][-1])\n",
    "print(object_file['validation'][0][-1])\n",
    "print(object_file['training'][1][-1])\n",
    "print(object_file['validation'][1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21964c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_range_aperture_2(img, min, max):\n",
    "    mask = aperture_generator(128, 50, 40)\n",
    "    array = np.ndarray.flatten(img[np.where(mask==1)])\n",
    "    img = img - np.amin(array)\n",
    "    img = img / (np.amax(array) - np.amin(array))\n",
    "    img = img * (max - min)\n",
    "    img += min\n",
    "    img[np.where(mask == 0)] = 0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43df7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "abr_list = [\"C10\", \"C12.x\", \"C12.y\", \"C21.x\", \"C21.y\", \"C23.x\", \"C23.y\", \"C30\", \n",
    "\"C32.x\", \"C32.y\", \"C34.x\", \"C34.y\"]\n",
    "\n",
    "abr_lim = [2e-6, 1.5e-6, 1.5e-6, 3e-5, 3e-5, 1e-5, 1e-5, 3e-4, 2e-4, 2e-4, 1.5e-4, 1.5e-4]\n",
    "abr_default = [2e-9, 2e-9, 2e-9, 20e-9, 20e-9, 20e-9, 20e-9, 0.5e-6, 1e-6, 1e-6, 1e-6, 1e-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af130035",
   "metadata": {},
   "outputs": [],
   "source": [
    "abr_coeff = 'C2'\n",
    "dwell_time = 250\n",
    "path = '/home/chenyu/Desktop/NionData/090921/250ms_linescan_Aligned/'\n",
    "# path = '/home/chenyu/Desktop/NionData/090221_50mrad_linescans/'\n",
    "file_list = glob.glob(path + abr_coeff +'*'+str(dwell_time)+'ms_bin1_repx5voaFOV.npy')\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ronch_list = np.load('NionRelated/C12.y_2e-06m_100steps_10ms_bin8.npy')\n",
    "nsteps = 100\n",
    "nrep = 5\n",
    "x_list = []\n",
    "y_list = []\n",
    "t_list = []\n",
    "coeff_list = []\n",
    "x_default = [(abr_default[i] + abr_lim[i]/2) / abr_lim[i] for i in range(len(abr_list))]\n",
    "\n",
    "for file in file_list:\n",
    "    name_list = file.replace('/','_').split('_')\n",
    "    print(file, name_list)\n",
    "    abr_coeff = name_list[-6]\n",
    "    idx = abr_list.index(abr_coeff)\n",
    "    abr_range = abr_lim[idx]\n",
    "    value_list = [(i - nsteps//2) * abr_range / nsteps for i in range(nsteps)]\n",
    "    ronch_list = np.load(file)\n",
    "    frame_list = []\n",
    "    pred = []\n",
    "    \n",
    "    for i in range(nsteps):\n",
    "        for j in range(nrep):\n",
    "            x = [x_default[j] for j in range(len(x_default))]\n",
    "            x[idx] = (value_list[i] + abr_range / 2) / abr_range\n",
    "            x_list.append(x)\n",
    "            frame = ronch_list[i * nrep + j,:,:]\n",
    "            frame = scale_range(frame, 0, 1)\n",
    "            new_channel = np.zeros(frame.shape)\n",
    "            img_stack = np.dstack((frame, new_channel, new_channel))\n",
    "            x = torch.tensor(np.transpose(img_stack)).to(device)\n",
    "            x = x.unsqueeze(0).float()\n",
    "            prediction = model(x)\n",
    "            pred.append(prediction[0][0].cpu().detach().numpy())\n",
    "            \n",
    "    y_list.append(np.array(pred))\n",
    "    t_list.append(name_list[-3])\n",
    "    coeff_list.append(abr_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = [8,6])\n",
    "y_list_avg = []\n",
    "x_list_avg = np.array(x_list[::5])\n",
    "std_list = np.zeros(nsteps)\n",
    "\n",
    "for i in range(len(t_list)):\n",
    "    temp = y_list[i].reshape([nsteps, 5])\n",
    "    y_list_avg.append(temp.mean(-1))\n",
    "    for j in range(nsteps):\n",
    "        std_list[j] = np.std(temp[j,1:])\n",
    "    p = ax.plot(np.array(value_list) * 1e6, temp[:,1:].mean(-1), linewidth = 2, label = coeff_list[i])\n",
    "#     p = ax.plot(np.array(value_list) * 1e6, temp.mean(-1), linewidth = 2, label = t_list[i])\n",
    "#         p = ax.plot(temp.mean(-1), linewidth = 2, label = coeff_list[i])\n",
    "    ax.fill_between(np.array(value_list) * 1e6, temp[:,1:].mean(-1) - np.array(std_list), \n",
    "                    temp.mean(-1) + np.array(std_list), alpha = 0.3)\n",
    "\n",
    "ax.set_xlabel('Aberration Coefficients (um)',fontsize = 16)\n",
    "ax.set_ylabel('Normalized Defocus + Emit', fontsize = 16)\n",
    "ax.tick_params(axis='x', labelsize=16)\n",
    "ax.tick_params(axis='y', labelsize=16)\n",
    "ax.legend(fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c9acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
