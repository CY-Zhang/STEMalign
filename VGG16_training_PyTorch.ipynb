{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.nn import Module\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized CNN model\n",
    "class Net(Module):   \n",
    "    def __init__(self, pretrained = False):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv7 = nn.Conv2d(256, 256, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv8 = nn.Conv2d(256, 512, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv9 = nn.Conv2d(512, 512, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv10 = nn.Conv2d(512, 512, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv11 = nn.Conv2d(512, 512, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv12 = nn.Conv2d(512, 512, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.conv13 = nn.Conv2d(512, 512, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
    "        self.fc1 = nn.Linear(4 * 4 * 512, 256)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "    \n",
    "    def lock_base(self):\n",
    "        for parameter in self.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        self.fc1.weight.requires_grad = True\n",
    "        self.fc1.bias.requires_grad = True\n",
    "        self.fc2.weight.requires_grad = True\n",
    "        self.fc2.bias.requires_grad = True\n",
    "    \n",
    "    def unlock_base(self):\n",
    "        for parameter in self.parameters():\n",
    "            parameter.requires_grad = True\n",
    "            \n",
    "    def load_pretrained(self):\n",
    "        print(\"Loading weights and bias from VGG16.\")\n",
    "        vgg16 = torchvision.models.vgg16(pretrained = True)\n",
    "        self.conv1.weight.data = vgg16.features[0].weight.data.to(device = device)\n",
    "        self.conv1.bias.data = vgg16.features[0].bias.data.to(device = device)\n",
    "        self.conv2.weight.data = vgg16.features[2].weight.data.to(device = device)\n",
    "        self.conv2.bias.data = vgg16.features[2].bias.data.to(device = device)\n",
    "        self.conv3.weight.data = vgg16.features[5].weight.data.to(device = device)\n",
    "        self.conv3.bias.data = vgg16.features[5].bias.data.to(device = device)\n",
    "        self.conv4.weight.data = vgg16.features[7].weight.data.to(device = device)\n",
    "        self.conv4.bias.data = vgg16.features[7].bias.data.to(device = device)\n",
    "        self.conv5.weight.data = vgg16.features[10].weight.data.to(device = device)\n",
    "        self.conv5.bias.data = vgg16.features[10].bias.data.to(device = device)\n",
    "        self.conv6.weight.data = vgg16.features[12].weight.data.to(device = device)\n",
    "        self.conv6.bias.data = vgg16.features[12].bias.data.to(device = device)\n",
    "        self.conv7.weight.data = vgg16.features[14].weight.data.to(device = device)\n",
    "        self.conv7.bias.data = vgg16.features[14].bias.data.to(device = device)\n",
    "        self.conv8.weight.data = vgg16.features[17].weight.data.to(device = device)\n",
    "        self.conv8.bias.data = vgg16.features[17].bias.data.to(device = device)\n",
    "        self.conv9.weight.data = vgg16.features[19].weight.data.to(device = device)\n",
    "        self.conv9.bias.data = vgg16.features[19].bias.data.to(device = device)\n",
    "        self.conv10.weight.data = vgg16.features[21].weight.data.to(device = device)\n",
    "        self.conv10.bias.data = vgg16.features[21].bias.data.to(device = device)\n",
    "        self.conv11.weight.data = vgg16.features[24].weight.data.to(device = device)\n",
    "        self.conv11.bias.data = vgg16.features[24].bias.data.to(device = device)\n",
    "        self.conv12.weight.data = vgg16.features[26].weight.data.to(device = device)\n",
    "        self.conv12.bias.data = vgg16.features[26].bias.data.to(device = device)\n",
    "        self.conv13.weight.data = vgg16.features[28].weight.data.to(device = device)\n",
    "        self.conv13.bias.data = vgg16.features[28].bias.data.to(device = device)\n",
    "    \n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = F.relu(self.conv10(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = F.relu(self.conv13(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the training data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/home/chenyu/Desktop/CNNmeasurement/TrainingData_CoarseCNN/'\n",
    "train_data = np.load(input_path + 'FullRandom_40mrad_highCs_3fold_C5negC1_C3negC1_whiteNoise_32pxGaussian_multiNoise_30pA_128pxRonch_x25000.npy')\n",
    "print(train_data.shape)\n",
    "\n",
    "train_label = np.load(input_path + 'FullRandom_40mrad_highCs_3fold_C5negC1_C3negC1_whiteNoise_32pxGaussian_multiNoise_30pA_newEmit_x25000.npy')\n",
    "train_label_1 = (train_label - np.amin(train_label))/(np.amax(train_label) - np.amin(train_label))\n",
    "print(train_label.shape)\n",
    "\n",
    "train_label = np.load(input_path + 'FullRandom_40mrad_highCs_3fold_C5negC1_C3negC1_whiteNoise_32pxGaussian_multiNoise_30pA_defocus_x25000.npy')\n",
    "train_label_2 = (train_label - np.amin(train_label))/(np.amax(train_label) - np.amin(train_label))\n",
    "print(train_label.shape)\n",
    "\n",
    "train_label = (train_label_1 + train_label_2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_range (input, min, max):\n",
    "    input += -(np.min(input))\n",
    "    input /= np.max(input) / (max - min)\n",
    "    input += min\n",
    "    return input    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RonchigramDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform = None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = scale_range(self.data[index,:,:].astype('float'), 0, 1)\n",
    "        new_channel = np.zeros(img.shape)\n",
    "        img = np.dstack((img, img, img))\n",
    "        img = Image.fromarray(np.uint8(img*255))\n",
    "        y_label = torch.tensor(float(self.labels[index]))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((135, 135)),\n",
    "            transforms.RandomResizedCrop((128, 128), scale = (0.9, 1.0), ratio = (0.97, 1.03)),\n",
    "            transforms.ToTensor(),\n",
    "            # TODO: need to add random shear here\n",
    "            # option to normalize a tensor with mean and standard deviation, similar to featurewise center in Keras\n",
    "#             transforms.Normalize((1.0, 1.0, 1.0), (1.0, 1.0, 1.0)),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 5e-5\n",
    "train_CNN = False\n",
    "batch_size = 20\n",
    "shuffle = True\n",
    "pin_memory = True\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RonchigramDataset(train_data, train_label, transform = transform)\n",
    "\n",
    "train_set, validation_set = torch.utils.data.random_split(dataset,[20000,5002])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers,pin_memory=pin_memory)\n",
    "\n",
    "validation_loader = DataLoader(dataset=validation_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the CNN model and start training\n",
    "model = Net().to(device)\n",
    "model.load_pretrained()\n",
    "# model.lock_base()\n",
    "criterion = nn.MSELoss(reduction = 'mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps = 1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader == train_loader:\n",
    "        print(\"Checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on validation data\")\n",
    "\n",
    "    sum_MSE = 0\n",
    "    counter = 0\n",
    "    loss = nn.MSELoss(reduction = 'mean')\n",
    "    model.eval()\n",
    "\n",
    "    y_list = torch.empty(0).to(device = device)\n",
    "    pred_list = torch.empty(0).to(device = device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            pred = torch.squeeze(model(x))\n",
    "            pred_list = torch.cat((pred_list, pred), 0)\n",
    "            y_list = torch.cat((y_list, y), 0)\n",
    "            sum_MSE += loss(pred, y)\n",
    "            counter += 1\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize = (5,5))\n",
    "    img = ax.scatter(y_list.cpu().numpy(), pred_list.cpu().numpy(), s = 1)\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.plot(np.linspace(0,1,100), np.linspace(0,1,100),'--', c = 'red')\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "    ax.set_xlabel('Truth',fontsize = 16)\n",
    "    ax.set_ylabel('Prediction', fontsize = 16)\n",
    "    plt.show()\n",
    "    \n",
    "    model.train()\n",
    "            \n",
    "    return f\"{float(sum_MSE)/float(counter):.4f}\"\n",
    "    print(\n",
    "            f\"Got accuracy {float(sum_MSE)/float(counter):.4f}\"\n",
    "        )\n",
    "#     model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-worry",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        loop = tqdm(train_loader, total = len(train_loader), leave = True)\n",
    "        if epoch % 2 == 0:\n",
    "            val_acc = check_accuracy(validation_loader, model)\n",
    "            print(val_acc)\n",
    "            loop.set_postfix(val_acc = val_acc)\n",
    "        for imgs, labels in loop:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loop.set_description(f\"Epoch [{epoch}/{num_epochs}]\")\n",
    "            loop.set_postfix(loss = loss.item())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02490b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7a585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(outputs, labels.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f40fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
